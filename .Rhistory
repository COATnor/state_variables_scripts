if(!all(as.data.frame(table(dat_new$event))$Freq == 8)) print(paste("needs checking", dat_name))
test <- dat_new %>% dplyr::group_by(v_image_name) %>%
dplyr::summarise(sum_pres = sum(v_presence, na.rm = TRUE))
## check that every image has only one classification
if(!all(test$sum_pres == 1)) print(paste("needs checking", dat_name))
unique(test$sum_pres)
View(test)
preprocess_classifications <- function(dat_name = dat_name, meta_name = meta_name, out_dir = out_dir, save = FALSE, keep_manual = "low_confidence", is.dir = TRUE) {
## load libraries
#if (!require('tidyverse', lib.loc = "/mnt/coat-ns8028k/Rlibs")) install.packages('tidyverse', lib = "/mnt/coat-ns8028k/Rlibs"); library('tidyverse', lib.loc = "/mnt/coat-ns8028k/Rlibs")
#if (!require('lubridate', lib.loc = "/mnt/coat-ns8028k/Rlibs")) install.packages('lubridate', lib = "/mnt/coat-ns8028k/Rlibs"); library('lubridate', lib.loc = "/mnt/coat-ns8028k/Rlibs")
library(tidyverse)
library(lubridate)
## clear workspace and free some memory (R will crash if all memory is used)
suppressWarnings(remove(list = c("dat_1", "dat_2", "dat_3", "dat_all", "dat_animal_empty_bad_quality", "dat_events_manual", "dat_motion", "dat_new",
"test", "events_manual", "events_same", "events_check", "images_manual", "images_same")))
gc(reset = TRUE)
## load data
if (is.dir) {
dat <- read.table(dat_name, header = TRUE, sep = ";")
meta <- read.table(meta_name, header = TRUE, sep = ";")
} else {
dat <- dat_name
meta <- meta_name
dat_name <- paste(dat$sn_locality[1], dat$t_date[nrow(dat)])
}
if ("corrupted image" %in% meta$v_comment) {
if (!all(meta$v_image_name %in% dat$v_image_name)) {
meta <- meta[-which(meta$v_comment == "corrupted image"),]
print(paste("images missing in classification file", dat_name))
}
}
if(!all(dat$v_image_name %in% meta$v_image_name)) {
print(paste("check", dat_name))
break
} else {
print(paste("processing", dat_name))
}
## preprocess data
dat_all <- left_join(meta, dat, by = c("sn_region", "sn_locality", "sn_section", "sc_type_of_sites_ecological", "sn_site", "t_date", "t_time", "v_image_name")) %>% # combine metadata and classifications
mutate(event = paste(v_trigger_mode, v_event, sn_site, sep = "_"))   # add unique event-id
dat_motion <- filter(dat_all, v_trigger_mode == "motion_sensor")  # only motion sensor images
## remove rawdata files and free some memory
remove(dat)
remove(meta)
gc(reset = TRUE)
## keep the image with manual classification ----------------
## keep only manual classification for images with low confidence or mustelid and quality check images for komag 2016-2018 (all images were classified manually)
if(grepl("komagdalen", dat_name) & grepl(c("2016|2017|2018"), dat_name) & keep_manual == "low_confidence") {
dat_motion$keep_manual <- ifelse(dat_motion$v_type_manual_classification %in% c("quality_check_random", "quality_check_classes", "quality_check_confidence_level", "mustelid"), "yes",
ifelse(dat_motion$v_presence_manual == 1 & dat_motion$v_confidence_automatic < 0.9, "yes", "no"))
events_manual <- unique(dat_motion$event[which(dat_motion$keep_manual == "yes")])
dat_events_manual <- dat_motion[which(dat_motion$event %in% events_manual & dat_motion$v_presence_manual == 1),]  # dataframe with all images with manual classification
} else {
events_manual <- dat_motion$event[which(dat_motion$v_presence_manual == 1)]  # all events that have a manual classification
dat_events_manual <- dat_motion[which(dat_motion$event %in% events_manual & dat_motion$v_presence_manual == 1),]  # dataframe with all images with manual classification
}
## images with one manual classification
images_1_manual <- dat_events_manual[!dat_events_manual$event %in% dat_events_manual$event[duplicated(dat_events_manual$event)],]$v_image_name
## if one image has and animal and the other is empty or has bad quality, keep the animal
dat_events_2_manual <- dat_events_manual[!dat_events_manual$v_image_name %in% images_1_manual,]  # all events with two manual classifications
events_animal_manual <- dat_events_2_manual$event[dat_events_2_manual$v_class_id %in% c("aves", "cricetidae", "lem_lem", "mus_erm", "mus_niv", "sor_sp")]  # all events with animal
events_empty_bad_manual <- dat_events_2_manual$event[dat_events_2_manual$v_class_id %in% c("empty", "bad_quality")]  # al events with empty and bad quality images
events_empty_animal_manual <- intersect(events_animal_manual, events_empty_bad_manual)  # events with one animal and one empty or bad qualit image
images_empty_animal_manual <- dat_events_2_manual$v_image_name[dat_events_2_manual$event %in% events_empty_animal_manual & dat_events_2_manual$v_class_id %in% c("aves", "cricetidae", "lem_lem", "mus_erm", "mus_niv", "sor_sp")] # events with one animal and one empty or bad qualit image
## keep the first image of all other images
dat_events_keep_first_manual <- dat_events_2_manual[!dat_events_2_manual$event %in% events_empty_animal_manual,]
images_keep_first_manual <- dat_events_keep_first_manual$v_image_name[!duplicated(dat_events_keep_first_manual$event)]
all(events_manual %in% c(events_empty_animal_manual, dat_events_keep_first_manual$event, dat_events_manual$event[dat_events_manual$v_image_name %in% images_1_manual]))
## keep the image with higher confidence if both image have the same class --------------
## only images without manual classification
dat_1 <- dat_motion[!dat_motion$event %in% events_manual,]
## all events where both images have the same classification
classes <- unique(dat_all$v_class_id)
events_same <- vector(mode = "list", length = length(classes))
for (i in 1:length(classes)) {
events_same[[i]] <- dat_1$event[which(dat_1$v_class_id == classes[i] & dat_1$v_presence_automatic == 1)]
events_same[[i]] <- events_same[[i]][duplicated(events_same[[i]])]
}
events_same <- unlist(events_same)
## keep images with higher confidence
images_same <- dat_1 %>% filter(event %in% events_same & !is.na(v_confidence_automatic)) %>%
group_by(event) %>%
slice(which.max(v_confidence_automatic)) %>%
arrange(v_image_name) %>%
ungroup() %>%
select(v_image_name) %>%
as.vector() %>%
unlist
## keep image with animal if only one image contains an animal --------------
dat_2 <- dat_1[!dat_1$event %in% events_same,]
## all events where the two images belong to different classes (e.g. empty and animal)
events_empty <- dat_2$event[which(dat_2$v_class_id == "empty" & dat_2$v_presence_automatic == 1)]  # all events where one of the image is empty
events_bad_quality <- dat_2$event[which(dat_2$v_class_id == "bad_quality" & dat_2$v_presence_automatic == 1)]  # all events where one of the images has bad quality
events_animal <- c()
classes <- classes[!classes %in% c("empty", "bad_quality")]
for (i in 1:length(classes)) {
events_animal[[i]] <- dat_2$event[which(dat_2$v_class_id == classes[i] & dat_2$v_presence_automatic == 1)]
}
events_animal <- unlist(events_animal) # all events where one of the images has an animal
events_animal_empty <- intersect(events_animal, events_empty)  # all events where one image is empty and the other has an animal
events_animal_bad_quality <- intersect(events_animal, events_bad_quality)  # all events where one image has bad quality and the other has an animal
events_empty_bad_quality <- intersect(events_empty, events_bad_quality) # all events where one image is empty and the other has bad quality
events_animal_animal <- events_animal[!events_animal %in% c(events_animal_empty, events_animal_bad_quality)]  # all images where the two images have been classified as two different animas
## keep the image that has an animal
dat_animal_empty_bad_quality <- dat_2[dat_2$event %in% c(events_animal_empty, events_animal_bad_quality),]
images_animal <- dat_animal_empty_bad_quality$v_image_name[which(dat_animal_empty_bad_quality$v_class_id %in% classes & dat_animal_empty_bad_quality$v_presence_automatic == 1)]
## keep the image with higher confidence if one images is empty and the other has bad quality
images_empty <- dat_2 %>% filter(event %in% events_empty_bad_quality & !is.na(v_confidence_automatic)) %>%
group_by(event) %>%
slice(which.max(v_confidence_automatic)) %>%
arrange(v_image_name) %>%
ungroup() %>%
select(v_image_name) %>%
as.vector() %>%
unlist
## keep the image with higher confidence if the images have been classified as different animals
images_animal_animal <- dat_2 %>% filter(event %in% events_animal_animal & !is.na(v_confidence_automatic)) %>%
group_by(event) %>%
slice(which.max(v_confidence_automatic)) %>%
arrange(v_image_name) %>%
ungroup() %>%
select(v_image_name) %>%
as.vector() %>%
unlist
## keep single images (if there is only one image per motion sensor trigger) --------------
dat_3 <- dat_2[!dat_2$event %in% c(events_animal_empty, events_animal_bad_quality, events_animal_animal, events_empty_bad_quality),]
temp <- as.data.frame(table(dat_3$event))
event_single <- temp$Var1[temp$Freq == 8]
images_single <- unique(dat_3$v_image_name[dat_3$event %in%event_single])
## keep only selected images ------------------
dat_new <- filter(dat_all, v_image_name %in% c(images_1_manual, images_empty_animal_manual, images_keep_first_manual, images_same, images_animal, images_empty, images_animal_animal, images_single) | v_trigger_mode == "time_lapse") %>%
dplyr::rename(v_comment_meta = v_comment.x, v_comment_classification = v_comment.y)
## check if all events are there
events <- unique(dat_all$event)
if (all(events %in% dat_new$event)) {
print(paste("everything correct:", dat_name))
} else {
missing <- dat_all %>% filter(event %in% events[!events %in% dat_new$event]) %>%
filter(!duplicated(v_image_name)) %>%
select(v_image_name, event, v_comment.y)
print(paste("missing images:", dat_name))
print(missing)
}
## Combine manual and automatic classification -----------------------------
if(grepl("komagdalen", dat_name) & grepl(c("2016|2017|2018"), dat_name) & keep_manual == "low_confidence") {
temp <- dat_new$v_image_name[which(grepl("manual_classification", dat_new$v_type_manual_classification) & dat_new$v_confidence_automatic >= 0.9)]
dat_new$v_presence_manual[dat_new$v_image_name %in% temp] <- NA
}
temp <- dat_new$v_image_name[which(dat_new$v_presence_manual == 1)]
dat_new$v_presence <- ifelse(dat_new$v_image_name %in% temp, dat_new$v_presence_manual, dat_new$v_presence_automatic)
## Some checking -----------------------------
## check if all images have a classification
events_check <- dat_new$event[which(dat_new$v_presence == 1)]
if (!length(events_check) == length(events)) {
temp <-  events[!events %in% events_check]
if (nrow(missing) != 0) temp <- temp[!temp %in% missing$event]
if(length(temp) != 0) {
print(paste("needs checking", dat_name))
}
}
## check if there are no duplicated events
if(!all(as.data.frame(table(dat_new$event))$Freq == 8)) print(paste("needs checking", dat_name))
test <- dat_new %>% dplyr::group_by(v_image_name) %>%
dplyr::summarise(sum_pres = sum(v_presence, na.rm = TRUE))
## check that every image has only one classification
if(!all(test$sum_pres == 1)) print(paste("needs checking", dat_name))
## reformat files ------------------------
dat_final <- dat_new %>% select(-c(v_presence_automatic, v_presence_manual, event, v_image_name_original)) %>%
select(sn_region, sn_locality, sn_section, sc_type_of_sites_ecological, sn_site, t_date, t_time, v_image_name, v_trigger_mode, v_sequence, v_event, v_temperature,
v_class_id, v_presence, v_confidence_automatic, v_observer_manual, v_type_manual_classification, v_comment_classification, v_comment_meta)
## Save file -----------------------------
if (save) {
file_name <- paste0("preprocessed_image_classifications_", unique(dat_new$sn_locality), "_", tail(substr(dat_new$t_date, 1, 4), 1), ".txt")
write.table(dat_final, paste(out_dir, file_name, sep = "/"), row.names = FALSE, sep = ";")
}
return(dat_final)
}
for (i in 1:length(classification_lb_list)) {
lb_processed[[i]] <- preprocess_classifications(dat_name = classification_lb_list[[i]], meta_name = metadata_lb_list[[i]], is.dir = FALSE)
lb_processed[[i]]$t_year <- sub(".*_(\\d{4})\\.parquet$", "\\1", names_class_lb[i])  # add year
}
lb_dat <- do.call(rbind, lb_processed)
## river valleys
rv_processed <- c()
for (i in 1:length(classification_rv_list)) {
rv_processed[[i]] <- preprocess_classifications(dat_name = classification_rv_list[[i]], meta_name = metadata_rv_list[[i]], is.dir = FALSE)
rv_processed[[i]]$t_year <- sub(".*_(\\d{4})\\.txt$", "\\1", names_class_rv[i])  # add year
}
rv_dat <- do.call(rbind, rv_processed)
## combine both files
dat_all <- rbind(lb_dat, rv_dat)
years <- unique(dat_all$t_year)
for (i in 1:length(years)) {
## filter data per year
dat_year <- filter(dat_all, t_year == years[i])
## calculate number of passings per week
dat_week <- dat_year %>% mutate(t_date = ymd(t_date)) %>%
filter(!is.na(t_date)) %>%
mutate(t_week = week(ymd(t_date)), t_year = year(t_date)) %>%
mutate(t_year_week = paste(t_year, t_week, sep = "_")) %>%
mutate(t_year_week = factor(t_year_week, levels = unique(t_year_week))) %>%
dplyr::group_by(sn_region, sn_locality, sc_type_of_sites_ecological, sn_site, t_year, t_week, t_year_week, v_class_id) %>%
dplyr::summarise(v_abundance = sum(v_presence, na.rm = TRUE)) %>%
arrange(sn_site, t_year_week)
## keep only mustelids
dat_mustelid <- dat_week %>%  filter(v_class_id %in% c("mus_erm", "mus_niv"))
## inculde t_year_week??
## one file per data year (or calender year)
## save the file to a temporary directory (necessary for uploading it)
#state_var_names[i] <- paste0("state_variable_snomelt_snowbed_", years[i], ".txt")
#write.table(snowmelt[[i]], paste(tempdir(), state_var_names[i], sep = "/"), row.names = FALSE, sep = ";")
#print(paste("state variable calculated and saved to temporary directory:", state_var_names[i]))
##
out.dir <- "C:/Users/hanna/Box/COAT/Modules/Small rodent module/state_variable_developement/mustelide_abundance/data"
new_name <- paste0("state_variable_mustelid_abundance_", years[i], ".txt")
write.table(dat_mustelid, paste(out.dir, new_name, sep = "/"), row.names = FALSE, sep = ";")
}
years <- unique(dat_all$t_year)
for (i in 1:length(years)) {
## filter data per year
dat_year <- filter(dat_all, t_year == years[i])
## calculate number of passings per week
dat_week <- dat_year %>% mutate(t_date = ymd(t_date)) %>%
filter(!is.na(t_date)) %>%
mutate(t_week = week(ymd(t_date)), t_year = year(t_date)) %>%
mutate(t_year_week = paste(t_year, t_week, sep = "_")) %>%
mutate(t_year_week = factor(t_year_week, levels = unique(t_year_week))) %>%
dplyr::group_by(sn_region, sn_locality, sc_type_of_sites_ecological, sn_site, t_year, t_week, t_year_week, v_class_id) %>%
dplyr::summarise(v_abundance = sum(v_presence, na.rm = TRUE)) %>%
arrange(sn_site, t_year_week)
## keep only mustelids
dat_mustelid <- dat_week %>%  filter(v_class_id %in% c("mus_erm", "mus_niv"))
## inculde t_year_week??
## one file per data year (or calender year)
## save the file to a temporary directory (necessary for uploading it)
#state_var_names[i] <- paste0("state_variable_snomelt_snowbed_", years[i], ".txt")
#write.table(snowmelt[[i]], paste(tempdir(), state_var_names[i], sep = "/"), row.names = FALSE, sep = ";")
#print(paste("state variable calculated and saved to temporary directory:", state_var_names[i]))
##
out.dir <- "C:/Users/hbo042/Box/COAT/Modules/Small rodent module/state_variable_developement/mustelide_abundance/data"
new_name <- paste0("state_variable_mustelid_abundance_", years[i], ".txt")
write.table(dat_mustelid, paste(out.dir, new_name, sep = "/"), row.names = FALSE, sep = ";")
}
ymd(dat_all$t_date)
xx <- ymd(dat_all$t_date)
is.na(xx)
which(is.na(xx))
dat_all[which(is.na(xx)),]
## clear workspace
rm(list = ls())
## load libraries, missing packages will be installed
if (!require("remotes")) install.packages("remotes")
if (!require("ckanr")) remotes::install_github("ropensci/ckanr"); library("ckanr")
if (!require("tidyverse")) install.packages("tidyverse"); library("tidyverse")
if (!require("lubridate")) install.packages("tidyverse"); library("lubridate")
if (!require("tidyquant")) install.packages("tidyquant"); library("tidyquant")
if (!require("ggpubr")) install.packages("ggpubr"); library("ggpubr")
## load data fraom BOX until state variable is on dataportal
in.dir <- "C:/Users/hbo042/Box/COAT/Modules/Small rodent module/state_variable_developement/mustelide_abundance/data"
dat <- map_dfr(dir(in.dir, full.names = TRUE), read.table, header = TRUE, sep = ";")
## calculate number of passings per week and habitat
dat_mean <- dat %>% group_by(sn_region, sc_type_of_sites_ecological, t_year, t_week, t_year_week, v_class_id) %>%
summarise(v_abundance_mean = mean(v_abundance, na.rm = TRUE))
## merge weekly data and means per habitat
dat_all <- full_join(dat, dat_mean)
## add date (one day for each week) to make smoothing work
dat_all$t_date <- make_date(year = dat_all$t_year) + weeks(dat_all$t_week)
dat_mean$t_date <- make_date(year = dat_mean$t_year) + weeks(dat_mean$t_week)
p <- dat_all %>% filter(sc_type_of_sites_ecological == "hummock_mire") %>%
ggplot(aes(x = t_date, y = v_abundance, colour = v_class_id)) +
geom_point(alpha = 0.2) +
scale_color_manual(values = c("#3f2a14", "darkgoldenrod2"), labels = c("Least Weasel", "Stoat")) +
ylim(0, quantile(dat_all$v_abundance, 0.999)) +
geom_ma(ma_fun=SMA, n=16, alpha = 0.7, linetype = 1) +
labs(x ="Time", y = "Passages per week") +
theme_classic()
p
## make plotting function
plot_mustelids <- function(dat_all = dat_all, dat_mean = dat_mean, habitat = habitat) {
## make title
plot_title <- ifelse(habitat == "snowbed", "Snowbed",
ifelse(habitat == "hummock_mire", "Hummock mire",
ifelse(habitat == "meadow", "Meadow",
ifelse(habitat == "heath_near", "Heath near", "Heath far"))))
## subset data
dat_all_plot <- dat_all %>% filter(sc_type_of_sites_ecological == habitat)
dat_mean_plot <- dat_mean %>% filter(sc_type_of_sites_ecological == habitat)
## make plot
p <- ggplot(dat_all_plot, aes(x = t_date, y = v_abundance, colour = v_class_id)) +
geom_point(alpha = 0.2) +
scale_color_manual(values=c("#3f2a14", "darkgoldenrod2"), labels = c("Stoat", "Least weasel")) +
ylim(0, quantile(dat_all$v_abundance, 0.999)) +
geom_ma(data = dat_mean_plot, aes(x=t_date, y=v_abundance_mean), ma_fun = SMA, n = 4, lwd = 1, lty = 1, alpha = 0.8) +
labs(x ="Time", y = "Passages per week", title = plot_title) +
theme_classic()+
theme(legend.title=element_blank())
return(p)
}
p_snow <- plot_mustelids(dat_all = dat_all, dat_mean = dat_mean, habitat = "snowbed")
p_hummock <- plot_mustelids(dat_all = dat_all, dat_mean = dat_mean, habitat = "hummock_mire")
p_meadow <- plot_mustelids(dat_all = dat_all, dat_mean = dat_mean, habitat = "meadow")
p_heath_near <- plot_mustelids(dat_all = dat_all, dat_mean = dat_mean, habitat = "heath_near")
p_heath_far <- plot_mustelids(dat_all = dat_all, dat_mean = dat_mean, habitat = "heath_far")
x11(height = 10, width = 15)
ggarrange(p_snow, p_hummock, common.legend = TRUE, ncol = 1, nrow = 2, legend = "right")
?SMA
?geom_ma
SMA(dat_all$v_abundance[dat_all$sc_type_of_sites_ecological == "hummock_mire"], n = 16)
View(dat_all)
View(dat_mean)
p <- dat_all %>% filter(sc_type_of_sites_ecological == "snowbed") %>%
ggplot(aes(x = t_date, y = v_abundance, colour = v_class_id)) +
geom_point(alpha = 0.2) +
scale_color_manual(values = c("#3f2a14", "darkgoldenrod2"), labels = c("Least Weasel", "Stoat")) +
ylim(0, quantile(dat_all$v_abundance, 0.999)) +
geom_ma(ma_fun=SMA, n=16, alpha = 0.7, linetype = 1) +
labs(x ="Time", y = "Passages per week") +
theme_classic()
p
View(dat_all)
p <- dat_all %>% filter(sc_type_of_sites_ecological == "snowbed") %>%
ggplot(aes(x = t_date, y = v_abundance, colour = sn_site)) +
geom_point(alpha = 0.2) +
#scale_color_manual(values = c("#3f2a14", "darkgoldenrod2"), labels = c("Least Weasel", "Stoat")) +
ylim(0, quantile(dat_all$v_abundance, 0.999)) +
geom_ma(ma_fun=SMA, n=16, alpha = 0.7, linetype = 1) +
labs(x ="Time", y = "Passages per week") +
theme_classic()
p
p
p <- dat_all %>% filter(sc_type_of_sites_ecological == "snowbed") %>%
ggplot(aes(x = t_date, y = v_abundance, colour = v_class_id)) +
geom_point(alpha = 0.2) +
#scale_color_manual(values = c("#3f2a14", "darkgoldenrod2"), labels = c("Least Weasel", "Stoat")) +
ylim(0, quantile(dat_all$v_abundance, 0.999)) +
geom_ma(ma_fun=SMA, n=16, alpha = 0.7, linetype = 1) +
labs(x ="Time", y = "Passages per week") +
theme_classic()
p
x11(height = 10, width = 15)
ggarrange(p_snow, p_hummock, common.legend = TRUE, ncol = 1, nrow = 2, legend = "right")
## clear workspace
rm(list = ls())
## load libraries, missing packages will be installed
if (!require("remotes")) install.packages("remotes")
if (!require("ckanr")) remotes::install_github("ropensci/ckanr"); library("ckanr")
if (!require("tidyverse")) install.packages("tidyverse"); library("tidyverse")
if (!require("lubridate")) install.packages("tidyverse"); library("lubridate")
if (!require("arrow")) install.packages("tidyverse"); library("arrow")
## setup the connection to the data portal
COAT_url <- "https://data.coat.no/" # write here the url to the COAT data portal
COAT_key <- Sys.getenv("API_coat") # write here your API key if you are a registered user, continue without API key if you are not registered
ckanr_setup(url = COAT_url, key = COAT_key) # set up the ckanr-API
## download functions from github
source("https://github.com/COATnor/data_management_scripts/blob/master/download_parquet_files_from_coat_data_portal.R?raw=TRUE")
source("https://github.com/COATnor/data_preprocessing_scripts/blob/master/function_preprocessing_image_classifications_small_mamma_camara_traps.R?raw=TRUE")
## download functions from github
source("https://github.com/COATnor/data_management_scripts/blob/master/download_parquet_files_from_coat_data_portal.R?raw=TRUE")
source("https://github.com/COATnor/data_preprocessing_scripts/blob/master/function_preprocessing_image_classifications_small_mamma_camara_traps.R?raw=TRUE")
download_coat_data <- function(COAT_key = COAT_key,
name = name,
version = version,
filenames = filenames,
store = "session",
out.dir = out.dir) {
## setup the connection to the data portal
ckanr_setup(url =  "https://data.coat.no", key = COAT_key)
## search for the dataset
pkg <- package_search(q = list(paste("name:", name, sep = "")), fq = list(paste("version:", version, sep = "")), include_private = TRUE)$results[[1]]
urls <- pkg$resources %>% sapply("[[", "url") # get the urls to the files included in the dataset
filenames_dataset <- pkg$resources %>% sapply("[[", "name") # get the filenames
## check if all files are available
if (!all(filenames %in% filenames_dataset)){
print("Error: files not found")
break
}
## urls of filenames that should be downloaded
chrono <- order(filenames_dataset)
filenames_dataset <- filenames_dataset[chrono]
urls <- urls[chrono]
urls2 <- urls[which(filenames_dataset %in% filenames)]
## download all files
mylist <- c() # empty object for the files
for (i in 1:length(filenames)) {
mylist[[i]] <- ckan_fetch(urls2[i],
store = store,
path = paste(out.dir, name, filenames[i], sep = "/"),
sep = ";",
header = TRUE,
format = "txt"
)
}
#myfile <- do.call(rbind, mylist)
return(mylist)
}
## define year(s) for which the data should be downloaded
years_ko <- 2016:2024  # komagdalen lemming blocks
years_vj <- 2019:2024  # vestre jakobselv lemming blocks
years_rv <- 2021:2024  # river valleys
## specify filenames
names_class_lb <- c(paste0("V_rodents_cameratraps_image_classification_lemming_blocks_komagdalen_", years_ko, ".parquet"),
paste0("V_rodents_cameratraps_image_classification_lemming_blocks_vestre_jakobselv_", years_vj, ".parquet"))
names_meta_lb <- c(paste0("V_rodents_cameratraps_image_metadata_lemming_blocks_komagdalen_", years_ko, ".parquet"),
paste0("V_rodents_cameratraps_image_metadata_lemming_blocks_vestre_jakobselv_", years_vj, ".parquet"))
names_class_rv <- c(paste0("V_rodents_cameratraps_image_classification_intensive_quadrats_komagdalen_", years_rv, ".parquet"),
paste0("V_rodents_cameratraps_image_classification_intensive_quadrats_vestre_jakobselv_", years_rv, ".parquet"))
names_meta_rv <- c(paste0("V_rodents_cameratraps_image_metadata_intensive_quadrats_komagdalen_", years_rv, ".parquet"),
paste0("V_rodents_cameratraps_image_metadata_intensive_quadrats_vestre_jakobselv_", years_rv, ".parquet"))
## download image classification and metadata for lemming blocks
classification_lb_list <- download_coat_parquet(COAT_key = Sys.getenv("api_COAT"),
name = "v_rodents_cameratraps_image_classification_lemming_blocks_v4",
version = 4,
filenames = names_class_lb)
metadata_lb_list <- download_coat_parquet(COAT_key = Sys.getenv("api_COAT"),
name = "v_rodents_cameratraps_image_metadata_lemming_blocks_v4",
version = 4,
filenames = names_meta_lb)
## download image classification and metadata for river valleys
classification_rv_list <- download_coat_parquet(COAT_key = Sys.getenv("api_COAT"),
name = "v_rodents_cameratraps_image_classification_intensive_quadrats_v3",
version = 3,
filenames = names_class_rv)
metadata_rv_list <- download_coat_parquet(COAT_key = Sys.getenv("api_COAT"),
name = "v_rodents_cameratraps_image_metadata_intensive_quadrats_v3",
version = 3,
filenames = names_meta_rv)
## lemming blocks
lb_processed <- c()
for (i in 1:length(classification_lb_list)) {
lb_processed[[i]] <- preprocess_classifications(dat_name = classification_lb_list[[i]], meta_name = metadata_lb_list[[i]], is.dir = FALSE)
lb_processed[[i]]$t_year <- sub(".*_(\\d{4})\\.parquet$", "\\1", names_class_lb[i])  # add year
}
lb_dat <- do.call(rbind, lb_processed)
## river valleys
rv_processed <- c()
for (i in 1:length(classification_rv_list)) {
rv_processed[[i]] <- preprocess_classifications(dat_name = classification_rv_list[[i]], meta_name = metadata_rv_list[[i]], is.dir = FALSE)
rv_processed[[i]]$t_year <- sub(".*_(\\d{4})\\.parquet$", "\\1", names_class_rv[i])  # add year
}
rv_dat <- do.call(rbind, rv_processed)
## combine both files
dat_all <- rbind(lb_dat, rv_dat)
years <- unique(dat_all$t_year)
state_var_names <- c()
for (i in 1:length(years)) {
## filter data per year
dat_year <- filter(dat_all, t_year == years[i])
## calculate number of passings per week
dat_week <- dat_year %>% mutate(t_date = ymd(t_date)) %>%
filter(!is.na(t_date)) %>%
mutate(t_week = week(ymd(t_date)), t_year = year(t_date)) %>%
mutate(t_year_week = paste(t_year, t_week, sep = "_")) %>%
mutate(t_year_week = factor(t_year_week, levels = unique(t_year_week))) %>%
dplyr::group_by(sn_region, sn_locality, sc_type_of_sites_ecological, sn_site, t_year, t_week, t_year_week, v_class_id) %>%
dplyr::summarise(v_abundance = sum(v_presence, na.rm = TRUE)) %>%
arrange(sn_site, t_year_week) %>%
ungroup() %>%
select(-t_year_week)
## keep only mustelids
dat_mustelid <- dat_week %>%  filter(v_class_id %in% c("mus_erm", "mus_niv"))
## save the file to a temporary directory (necessary for uploading it)
state_var_names[i] <- paste0("V37_specialist_predators_mustelid_abundance_", years[i], ".txt")
write.table(dat_mustelid, paste(tempdir(), state_var_names[i], sep = "/"), row.names = FALSE, sep = ";")
print(paste("state variable calculated and saved to temporary directory:", state_var_names[i]))
##
#out.dir <- "C:/Users/hbo042/Box/COAT/Modules/Small rodent module/state_variable_developement/mustelide_abundance/data"
#new_name <- paste0("state_variable_mustelid_abundance_", years[i], ".txt")
#write.table(dat_mustelid, paste(out.dir, new_name, sep = "/"), row.names = FALSE, sep = ";")
}
## serach for your dataset
state_name <- "v37_specialist_predators_mustelid_abundance_v1" # write here the name including the version of the state variable you want to add data to
state_version <- "1" # write here the version of the state variable
pkg_state <- package_search(q = list(paste("name:", state_name, sep = "")), fq = list(paste("version:", state_version, sep = "")), include_private = TRUE, include_drafts = TRUE)$results[[1]] # search for the dataset and save the results
pkg_state$name # check the name
## serach for your dataset
state_name <- "v37_specialist_predators_mustelid_abundance_v1" # write here the name including the version of the state variable you want to add data to
state_version <- "1" # write here the version of the state variable
pkg_state <- package_search(q = list(paste("name:", state_name, sep = "")), fq = list(paste("version:", state_version, sep = "")), include_private = TRUE, include_drafts = TRUE)$results[[1]] # search for the dataset and save the results
filenames_state <- pkg_state$resources %>% sapply("[[", "name") # get the filenames
filenames_state # are there any files
state_var_names
for (i in state_var_names) {
resource_create(
package_id = pkg_state$id,
description = NULL,
upload = paste(tempdir(), i, sep = "/"),
name = i,
http_method = "POST"
)
}
pkg_state$name # check that the name is correct
## save metadata of the package as a list (as = table -> but the object will be a list)
pkg_updated <- package_show(pkg_state$id, as = "table", http_method = "POST")
## do the necessary modifications of the metadata
names(pkg_updated) # show the names of all metadata fields that can be updated
pkg_updated
pkg_updated$scientific_name
## do the necessary modifications of the metadata
names(pkg_updated) # show the names of all metadata fields that can be updated
pkg_updated$private <- TRUE # set private = FALSE to publish a dataset
pkg_updated$state <- "active"
pkg_updated$scientific_name <- "Mustela erminea,Mustela nivalis"
pkg_updated$author
pkg_updated$author <- "Eeva Soininen,eeva.soininen@uit.no"
pkg_updated$author
pkg_updated$author <- "Eeva Soininen"
## discard empty metadata fields (they will cause a validation error)
pkg_updated <- discard(pkg_updated, is.null)
## update the package
package_update(pkg_updated, pkg_state$id, http_method = "POST")
pkg_updated$author
